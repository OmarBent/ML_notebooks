{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a95ba0b5",
   "metadata": {},
   "source": [
    "### Cora dataset loader\n",
    "\n",
    "I use the well-known pytorch geometric to load and pre-process the Cora dataset. \n",
    "\n",
    "The dataset consists of 2708 Machine Learning publications classified into seven classes (i.e., ML topic). \n",
    "\n",
    "The node representation is a bag-of-words feature vector of dimension 1433. 1433 is the size of a pre-built dictionary of unique words (collected from all papers in the dataset). Each dimension of the (normalized) feature vector indicates the absence/presence of the corresponding word from the dictionary. \n",
    "\n",
    "Further, two documents are connected if there exists a citation link between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "929b4ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Dataset info ====\n",
      "Dataset: Cora():\n",
      "Number of features: 1433\n",
      "Number of classes: 7\n",
      "Number of nodes = 2708\n",
      "Number of edges = 10556\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())\n",
    "\n",
    "print(f'==== Dataset info ====')\n",
    "print(f'Dataset: {dataset}:')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "print(f'Number of nodes = {data.num_nodes}')\n",
    "print(f'Number of edges = {data.num_edges}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac2cfd3",
   "metadata": {},
   "source": [
    "### Training a GNN on the node classification task\n",
    "Following is the GCN model which is based on the GCNConv layer as defined in [this paper](https://arxiv.org/abs/1609.02907)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b301c8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(1433, 64)\n",
      "  (conv2): GCNConv(64, 7)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(99)\n",
    "        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb9a807",
   "metadata": {},
   "source": [
    "##### Training\n",
    "Next I train the model for 100 epochs using Adam, a cross-entropy loss function, and a hidden-layer of size 64 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dd2b446c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.9459\n",
      "Epoch: 002, Loss: 1.9349\n",
      "Epoch: 003, Loss: 1.9220\n",
      "Epoch: 004, Loss: 1.9034\n",
      "Epoch: 005, Loss: 1.8840\n",
      "Epoch: 006, Loss: 1.8627\n",
      "Epoch: 007, Loss: 1.8372\n",
      "Epoch: 008, Loss: 1.8158\n",
      "Epoch: 009, Loss: 1.7804\n",
      "Epoch: 010, Loss: 1.7525\n",
      "Epoch: 011, Loss: 1.7291\n",
      "Epoch: 012, Loss: 1.6872\n",
      "Epoch: 013, Loss: 1.6664\n",
      "Epoch: 014, Loss: 1.6238\n",
      "Epoch: 015, Loss: 1.5680\n",
      "Epoch: 016, Loss: 1.5415\n",
      "Epoch: 017, Loss: 1.4953\n",
      "Epoch: 018, Loss: 1.4602\n",
      "Epoch: 019, Loss: 1.4221\n",
      "Epoch: 020, Loss: 1.3935\n",
      "Epoch: 021, Loss: 1.3299\n",
      "Epoch: 022, Loss: 1.2709\n",
      "Epoch: 023, Loss: 1.2322\n",
      "Epoch: 024, Loss: 1.2015\n",
      "Epoch: 025, Loss: 1.1567\n",
      "Epoch: 026, Loss: 1.0978\n",
      "Epoch: 027, Loss: 1.0719\n",
      "Epoch: 028, Loss: 1.0344\n",
      "Epoch: 029, Loss: 0.9860\n",
      "Epoch: 030, Loss: 0.9684\n",
      "Epoch: 031, Loss: 0.9192\n",
      "Epoch: 032, Loss: 0.8783\n",
      "Epoch: 033, Loss: 0.8499\n",
      "Epoch: 034, Loss: 0.8134\n",
      "Epoch: 035, Loss: 0.8067\n",
      "Epoch: 036, Loss: 0.7590\n",
      "Epoch: 037, Loss: 0.6944\n",
      "Epoch: 038, Loss: 0.6844\n",
      "Epoch: 039, Loss: 0.6591\n",
      "Epoch: 040, Loss: 0.6249\n",
      "Epoch: 041, Loss: 0.6149\n",
      "Epoch: 042, Loss: 0.5879\n",
      "Epoch: 043, Loss: 0.5625\n",
      "Epoch: 044, Loss: 0.5705\n",
      "Epoch: 045, Loss: 0.5237\n",
      "Epoch: 046, Loss: 0.5075\n",
      "Epoch: 047, Loss: 0.4941\n",
      "Epoch: 048, Loss: 0.4713\n",
      "Epoch: 049, Loss: 0.4750\n",
      "Epoch: 050, Loss: 0.4691\n",
      "Epoch: 051, Loss: 0.4632\n",
      "Epoch: 052, Loss: 0.4727\n",
      "Epoch: 053, Loss: 0.4350\n",
      "Epoch: 054, Loss: 0.3950\n",
      "Epoch: 055, Loss: 0.3842\n",
      "Epoch: 056, Loss: 0.3930\n",
      "Epoch: 057, Loss: 0.3854\n",
      "Epoch: 058, Loss: 0.3834\n",
      "Epoch: 059, Loss: 0.3803\n",
      "Epoch: 060, Loss: 0.3771\n",
      "Epoch: 061, Loss: 0.3741\n",
      "Epoch: 062, Loss: 0.3614\n",
      "Epoch: 063, Loss: 0.3570\n",
      "Epoch: 064, Loss: 0.3490\n",
      "Epoch: 065, Loss: 0.3661\n",
      "Epoch: 066, Loss: 0.3518\n",
      "Epoch: 067, Loss: 0.3410\n",
      "Epoch: 068, Loss: 0.3347\n",
      "Epoch: 069, Loss: 0.3236\n",
      "Epoch: 070, Loss: 0.3132\n",
      "Epoch: 071, Loss: 0.3134\n",
      "Epoch: 072, Loss: 0.3114\n",
      "Epoch: 073, Loss: 0.2953\n",
      "Epoch: 074, Loss: 0.3049\n",
      "Epoch: 075, Loss: 0.2956\n",
      "Epoch: 076, Loss: 0.3014\n",
      "Epoch: 077, Loss: 0.3052\n",
      "Epoch: 078, Loss: 0.3029\n",
      "Epoch: 079, Loss: 0.2889\n",
      "Epoch: 080, Loss: 0.2751\n",
      "Epoch: 081, Loss: 0.2880\n",
      "Epoch: 082, Loss: 0.2809\n",
      "Epoch: 083, Loss: 0.2942\n",
      "Epoch: 084, Loss: 0.2739\n",
      "Epoch: 085, Loss: 0.2749\n",
      "Epoch: 086, Loss: 0.2641\n",
      "Epoch: 087, Loss: 0.2568\n",
      "Epoch: 088, Loss: 0.2406\n",
      "Epoch: 089, Loss: 0.2510\n",
      "Epoch: 090, Loss: 0.2395\n",
      "Epoch: 091, Loss: 0.2715\n",
      "Epoch: 092, Loss: 0.2449\n",
      "Epoch: 093, Loss: 0.2437\n",
      "Epoch: 094, Loss: 0.2679\n",
      "Epoch: 095, Loss: 0.2674\n",
      "Epoch: 096, Loss: 0.2278\n",
      "Epoch: 097, Loss: 0.2387\n",
      "Epoch: 098, Loss: 0.2556\n",
      "Epoch: 099, Loss: 0.2288\n",
      "Epoch: 100, Loss: 0.2320\n"
     ]
    }
   ],
   "source": [
    "model = GCN(hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def trainGCN():\n",
    "      model.train()\n",
    "      optimizer.zero_grad() \n",
    "      out = model(data.x, data.edge_index) \n",
    "      loss = criterion(out[data.train_mask], data.y[data.train_mask]) \n",
    "      loss.backward()  \n",
    "      optimizer.step() \n",
    "      return loss\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = trainGCN()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be9d096",
   "metadata": {},
   "source": [
    "### Check test accuracy on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fec344b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.816\n"
     ]
    }
   ],
   "source": [
    "def testGCN():\n",
    "      model.eval()\n",
    "      out = model(data.x, data.edge_index)\n",
    "      pred = out.argmax(dim=1)  \n",
    "      test_correct = pred[data.test_mask] == data.y[data.test_mask] \n",
    "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  \n",
    "      return test_acc\n",
    "\n",
    "test_acc = testGCN()\n",
    "print(f'Test Accuracy: {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83339882",
   "metadata": {},
   "source": [
    "### Training a MLP\n",
    "Here is a basic MLP baseline model to which we can compare the performance of GCN. Note that it only operates on input node features. \n",
    "\n",
    "GCNConv layers are replaced by Linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c0655482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (lin1): Linear(in_features=1433, out_features=64, bias=True)\n",
      "  (lin2): Linear(in_features=64, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(99)\n",
    "        self.lin1 = Linear(dataset.num_features, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "model = MLP(hidden_channels=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef604a4c",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "For fair comparison with the GCN model, I used the same optimizer and loss function as well as similar hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9e783882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.9487\n",
      "Epoch: 002, Loss: 1.9428\n",
      "Epoch: 003, Loss: 1.9370\n",
      "Epoch: 004, Loss: 1.9252\n",
      "Epoch: 005, Loss: 1.9165\n",
      "Epoch: 006, Loss: 1.9018\n",
      "Epoch: 007, Loss: 1.8876\n",
      "Epoch: 008, Loss: 1.8699\n",
      "Epoch: 009, Loss: 1.8528\n",
      "Epoch: 010, Loss: 1.8347\n",
      "Epoch: 011, Loss: 1.7980\n",
      "Epoch: 012, Loss: 1.7851\n",
      "Epoch: 013, Loss: 1.7552\n",
      "Epoch: 014, Loss: 1.7110\n",
      "Epoch: 015, Loss: 1.6771\n",
      "Epoch: 016, Loss: 1.6514\n",
      "Epoch: 017, Loss: 1.6263\n",
      "Epoch: 018, Loss: 1.5816\n",
      "Epoch: 019, Loss: 1.5288\n",
      "Epoch: 020, Loss: 1.4921\n",
      "Epoch: 021, Loss: 1.4151\n",
      "Epoch: 022, Loss: 1.4242\n",
      "Epoch: 023, Loss: 1.3401\n",
      "Epoch: 024, Loss: 1.2995\n",
      "Epoch: 025, Loss: 1.2481\n",
      "Epoch: 026, Loss: 1.1687\n",
      "Epoch: 027, Loss: 1.1455\n",
      "Epoch: 028, Loss: 1.0847\n",
      "Epoch: 029, Loss: 1.0007\n",
      "Epoch: 030, Loss: 0.9831\n",
      "Epoch: 031, Loss: 0.9263\n",
      "Epoch: 032, Loss: 0.8724\n",
      "Epoch: 033, Loss: 0.8298\n",
      "Epoch: 034, Loss: 0.7901\n",
      "Epoch: 035, Loss: 0.7319\n",
      "Epoch: 036, Loss: 0.7021\n",
      "Epoch: 037, Loss: 0.6447\n",
      "Epoch: 038, Loss: 0.6146\n",
      "Epoch: 039, Loss: 0.5788\n",
      "Epoch: 040, Loss: 0.5327\n",
      "Epoch: 041, Loss: 0.4962\n",
      "Epoch: 042, Loss: 0.5018\n",
      "Epoch: 043, Loss: 0.4391\n",
      "Epoch: 044, Loss: 0.4365\n",
      "Epoch: 045, Loss: 0.3971\n",
      "Epoch: 046, Loss: 0.3780\n",
      "Epoch: 047, Loss: 0.3461\n",
      "Epoch: 048, Loss: 0.3461\n",
      "Epoch: 049, Loss: 0.3191\n",
      "Epoch: 050, Loss: 0.2932\n",
      "Epoch: 051, Loss: 0.3223\n",
      "Epoch: 052, Loss: 0.2783\n",
      "Epoch: 053, Loss: 0.2759\n",
      "Epoch: 054, Loss: 0.2908\n",
      "Epoch: 055, Loss: 0.2552\n",
      "Epoch: 056, Loss: 0.2686\n",
      "Epoch: 057, Loss: 0.2364\n",
      "Epoch: 058, Loss: 0.2450\n",
      "Epoch: 059, Loss: 0.2547\n",
      "Epoch: 060, Loss: 0.2219\n",
      "Epoch: 061, Loss: 0.2514\n",
      "Epoch: 062, Loss: 0.2475\n",
      "Epoch: 063, Loss: 0.2310\n",
      "Epoch: 064, Loss: 0.2434\n",
      "Epoch: 065, Loss: 0.2452\n",
      "Epoch: 066, Loss: 0.2013\n",
      "Epoch: 067, Loss: 0.2113\n",
      "Epoch: 068, Loss: 0.2156\n",
      "Epoch: 069, Loss: 0.2158\n",
      "Epoch: 070, Loss: 0.2117\n",
      "Epoch: 071, Loss: 0.2141\n",
      "Epoch: 072, Loss: 0.2310\n",
      "Epoch: 073, Loss: 0.2063\n",
      "Epoch: 074, Loss: 0.2131\n",
      "Epoch: 075, Loss: 0.1972\n",
      "Epoch: 076, Loss: 0.2064\n",
      "Epoch: 077, Loss: 0.1938\n",
      "Epoch: 078, Loss: 0.2216\n",
      "Epoch: 079, Loss: 0.2097\n",
      "Epoch: 080, Loss: 0.1862\n",
      "Epoch: 081, Loss: 0.2048\n",
      "Epoch: 082, Loss: 0.1886\n",
      "Epoch: 083, Loss: 0.1962\n",
      "Epoch: 084, Loss: 0.1992\n",
      "Epoch: 085, Loss: 0.1751\n",
      "Epoch: 086, Loss: 0.1821\n",
      "Epoch: 087, Loss: 0.1907\n",
      "Epoch: 088, Loss: 0.1960\n",
      "Epoch: 089, Loss: 0.1801\n",
      "Epoch: 090, Loss: 0.2006\n",
      "Epoch: 091, Loss: 0.1814\n",
      "Epoch: 092, Loss: 0.1878\n",
      "Epoch: 093, Loss: 0.1783\n",
      "Epoch: 094, Loss: 0.1807\n",
      "Epoch: 095, Loss: 0.1584\n",
      "Epoch: 096, Loss: 0.1629\n",
      "Epoch: 097, Loss: 0.1462\n",
      "Epoch: 098, Loss: 0.1893\n",
      "Epoch: 099, Loss: 0.1500\n",
      "Epoch: 100, Loss: 0.1491\n"
     ]
    }
   ],
   "source": [
    "model = MLP(hidden_channels=64)\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  \n",
    "\n",
    "def trainMLP():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  \n",
    "      out = model(data.x)  \n",
    "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  \n",
    "      loss.backward()  \n",
    "      optimizer.step()  \n",
    "      return loss\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = trainMLP()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa62028",
   "metadata": {},
   "source": [
    "##### MLP test accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a3ff0ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5900\n"
     ]
    }
   ],
   "source": [
    "def testMLP():\n",
    "      model.eval()\n",
    "      out = model(data.x)\n",
    "      pred = out.argmax(dim=1)  \n",
    "      test_correct = pred[data.test_mask] == data.y[data.test_mask] \n",
    "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
    "      return test_acc\n",
    "\n",
    "test_acc = testMLP()\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72111936",
   "metadata": {},
   "source": [
    "# Answering question #4\n",
    "\n",
    "#### MLP vs. GCN model comparison \n",
    "\n",
    "GCN significantly outperformed MLP with more than 20% of classification accuracy. As expected, GCN are more suitable in handling graph data.\n",
    "\n",
    "#### Training GCN with more dimensions/layers\n",
    "\n",
    "In the following, I define GCN_plus which has an additional message passing layer compared to the previous GCN. I increased the dimension of the first hidden layer to 256 and set the second to 64. \n",
    "\n",
    "##### Explanations:\n",
    "After running the new GCN_plus, we can observe that it learns faster than the previous GCN. GCN_plus needed only 20 epochs to train while GCN needed 100 (training objective was around 0.2 and test accuracy was similar in both cases, i.e, ~81.5%). However, training GCN_plus for 100 epochs has led to overfitting and test accuracy has dropped to ~79%. \n",
    "\n",
    "Cora dataset is relatively small (~2700 samples) and only 5% of it is used for training (140 samples). Hence no need for a deeper network to improve the classification performance.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "de211264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN_plus(\n",
      "  (conv1): GCNConv(1433, 256)\n",
      "  (conv2): GCNConv(256, 64)\n",
      "  (conv3): GCNConv(64, 7)\n",
      ")\n",
      "Epoch: 001, Loss: 1.9461\n",
      "Epoch: 002, Loss: 1.9366\n",
      "Epoch: 003, Loss: 1.9192\n",
      "Epoch: 004, Loss: 1.8829\n",
      "Epoch: 005, Loss: 1.8501\n",
      "Epoch: 006, Loss: 1.7981\n",
      "Epoch: 007, Loss: 1.7434\n",
      "Epoch: 008, Loss: 1.6468\n",
      "Epoch: 009, Loss: 1.5595\n",
      "Epoch: 010, Loss: 1.4614\n",
      "Epoch: 011, Loss: 1.3250\n",
      "Epoch: 012, Loss: 1.1640\n",
      "Epoch: 013, Loss: 1.0587\n",
      "Epoch: 014, Loss: 0.8973\n",
      "Epoch: 015, Loss: 0.7660\n",
      "Epoch: 016, Loss: 0.6445\n",
      "Epoch: 017, Loss: 0.5792\n",
      "Epoch: 018, Loss: 0.4742\n",
      "Epoch: 019, Loss: 0.3859\n",
      "Epoch: 020, Loss: 0.3351\n",
      "Epoch: 021, Loss: 0.2629\n",
      "Test Accuracy: 0.818\n"
     ]
    }
   ],
   "source": [
    "class GCN_plus(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels_1, hidden_channels_2):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(99)\n",
    "        self.conv1 = GCNConv(dataset.num_features, hidden_channels_1)\n",
    "        self.conv2 = GCNConv(hidden_channels_1, hidden_channels_2)\n",
    "        self.conv3 = GCNConv(hidden_channels_2, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GCN_plus(hidden_channels_1=256, hidden_channels_2=64)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, 22):\n",
    "    loss = trainGCN()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "    \n",
    "test_acc = testGCN()\n",
    "print(f'Test Accuracy: {test_acc:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
