{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "RNN implementation from scratch with application to text generation."
      ],
      "metadata": {
        "id": "etyS8yM-1ZQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "\n",
        "    super().__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "    self.in2h = nn.Linear(self.input_dim, self.hidden_dim, bias=False)\n",
        "    self.h2h = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "    self.h2out = nn.Linear(self.hidden_dim, self.output_dim)\n",
        "\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, x, h0):\n",
        "\n",
        "    x = self.in2h(x)\n",
        "    h = self.h2h(h0)\n",
        "    h_new = torch.tanh(h + x)\n",
        "    out = self.h2out(h_new)\n",
        "\n",
        "    return out, h_new"
      ],
      "metadata": {
        "id": "kisiNpHRHLKA"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class text_data(Dataset):\n",
        "\n",
        "  def __init__(self, text, seq_length):\n",
        "\n",
        "    self.seq_length = seq_length\n",
        "\n",
        "    self.vocab = sorted(list(set(text)))\n",
        "    self.vocab_size = len(self.vocab)\n",
        "    self.text_size = len(text)\n",
        "\n",
        "    self.idx2char = {i:ch for i, ch in enumerate(self.vocab)}\n",
        "    self.char2idx = {ch:i for i, ch in enumerate(self.vocab)}\n",
        "\n",
        "    self.input = self.text2vect(text)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(len(self.input) / self.seq_length - 1) # -1 to account for the shift between X and Y\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    start_idx = idx * self.seq_length\n",
        "    end_idx = (idx+1) * self.seq_length\n",
        "\n",
        "    X = torch.tensor(self.input[start_idx:end_idx]).float()\n",
        "    Y = torch.tensor(self.input[start_idx+1:end_idx+1]).float()\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "  def text2vect(self, text):\n",
        "    return [ self.char2idx[text[i]] for i in range(len(text)) ]\n",
        "\n",
        "  def vect2text(self, vect):\n",
        "    string = \"\"\n",
        "    for i in vect:\n",
        "        string += self.idx2char[i]\n",
        "    return string"
      ],
      "metadata": {
        "id": "VL44ZoCPMuOz"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def generate_text(model: RNN, dataset: text_data, prediction_length: int = 100, hidden_dim=256) -> str:\n",
        "    \"\"\"\n",
        "    Generate text up to prediction_length characters\n",
        "    This function requires the dataset as argument in order to properly\n",
        "    generate the text and return the output as strings\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predicted = dataset.vect2text([random.randint(0, len(dataset.vocab) -1)])\n",
        "    hidden = torch.zeros(1, hidden_dim)\n",
        "\n",
        "    for i in range(prediction_length - 1):\n",
        "        last_char = torch.Tensor([dataset.char2idx[predicted[-1]]])\n",
        "        X, hidden = last_char.to(device), hidden.to(device)\n",
        "        out, hidden = model(X, hidden)\n",
        "        result = torch.multinomial(nn.functional.softmax(out, 1), 1).item()\n",
        "        predicted += dataset.idx2char[result]\n",
        "\n",
        "    return predicted"
      ],
      "metadata": {
        "id": "2tdHOm17l_eS"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "import torch\n",
        "\n",
        "\n",
        "hidden_dim = 256\n",
        "\n",
        "lr = 0.001\n",
        "epochs = 1000\n",
        "batch_size = 16\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using '{device}' device\")\n",
        "\n",
        "# Load data from text file\n",
        "file = open(\"/content/sample_data/data.txt\", \"r\")\n",
        "data = file.read()\n",
        "data = data.lower()\n",
        "file.close()\n",
        "\n",
        "print(data)\n",
        "# Dataloder\n",
        "training_data = text_data(data, seq_length=25)\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define model\n",
        "model = RNN(1, hidden_dim, training_data.vocab_size)\n",
        "model.to(device)\n",
        "\n",
        "model.train()\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optim = Adam(model.parameters(), lr=lr)\n",
        "\n",
        "train_losses = []\n",
        "for epoch in range(epochs):\n",
        "  epoch_losses = []\n",
        "  for x, y in train_dataloader:\n",
        "    if x.shape[0] != batch_size:\n",
        "      continue\n",
        "\n",
        "    hidden = torch.zeros(batch_size, hidden_dim)\n",
        "\n",
        "    x, y , hidden = x.to(device), y.to(device), hidden.to(device)\n",
        "    model.zero_grad()\n",
        "\n",
        "    loss = 0\n",
        "    for c in range(x.shape[1]):\n",
        "      y_hat, hidden = model(x[:, c].reshape(x.shape[0], 1), hidden)\n",
        "      l = loss_fn(y_hat, y[:, c].long())\n",
        "      loss += l\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
        "    optim.step()\n",
        "\n",
        "    epoch_losses.append(loss.detach().item() / x.shape[1])\n",
        "\n",
        "\n",
        "  train_losses.append(torch.tensor(epoch_losses).mean())\n",
        "  print(f\"Epoch '{epoch} -- Training loss = '{train_losses[epoch]}'\")\n",
        "  print(generate_text(model, train_dataloader.dataset))"
      ],
      "metadata": {
        "id": "YDqdrA9RTuz8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}